{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_hBXU97Qchb"
      },
      "source": [
        "***SAIDL PROJECT***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmYgXXOFQaX6"
      },
      "source": [
        "# RUN THIS FIRST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfM7sf52PsNd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "!git clone https://github.com/facebookresearch/DiT.git\n",
        "import DiT, os\n",
        "os.chdir('DiT')\n",
        "os.environ['PYTHONPATH'] = '/env/python:/content/DiT'\n",
        "!pip install diffusers timm --upgrade\n",
        "# DiT imports:\n",
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "from diffusion import create_diffusion\n",
        "from diffusers.models import AutoencoderKL\n",
        "from download import find_model\n",
        "from models import DiT_XL_2\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "torch.set_grad_enabled(False)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cpu\":\n",
        "    print(\"GPU not found. Using CPU instead.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjEGZEjuYdJH"
      },
      "source": [
        "# **CORE ML**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taWo6fr7F_PP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Loss functions ===\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=1.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        logp = F.log_softmax(inputs, dim=1)\n",
        "        p = torch.exp(logp)\n",
        "        logp = logp.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
        "        p = p.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
        "        loss = -self.alpha * ((1 - p) ** self.gamma) * logp\n",
        "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
        "\n",
        "class NormalizedCrossEntropy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NormalizedCrossEntropy, self).__init__()\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        log_probs = torch.log(probs + 1e-10)\n",
        "        loss = -log_probs[range(len(targets)), targets]\n",
        "        normalizer = -torch.sum(probs * log_probs, dim=1)\n",
        "        return (loss / (normalizer + 1e-10)).mean()\n",
        "\n",
        "class NormalizedFocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super(NormalizedFocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        log_probs = torch.log(probs + 1e-10)\n",
        "        pt = probs[range(len(targets)), targets]\n",
        "        log_pt = log_probs[range(len(targets)), targets]\n",
        "        loss = -(1 - pt) ** self.gamma * log_pt\n",
        "        normalizer = -torch.sum(probs * log_probs, dim=1)\n",
        "        return (loss / (normalizer + 1e-10)).mean()\n",
        "\n",
        "class ReverseCrossEntropy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ReverseCrossEntropy, self).__init__()\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float().to(logits.device)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        log_targets = torch.log(targets_one_hot + 1e-10)\n",
        "        loss = -torch.sum(probs * log_targets, dim=1)\n",
        "        return loss.mean()\n",
        "\n",
        "class ActivePassiveLoss(nn.Module):\n",
        "    def __init__(self, active_loss, passive_loss, alpha=0.5):\n",
        "        super(ActivePassiveLoss, self).__init__()\n",
        "        self.active_loss = active_loss\n",
        "        self.passive_loss = passive_loss\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        loss_active = self.active_loss(logits, targets)\n",
        "        loss_passive = self.passive_loss(logits, targets)\n",
        "        return self.alpha * loss_active + (1 - self.alpha) * loss_passive\n",
        "\n",
        "# === Dataset with noise ===\n",
        "\n",
        "class NoisyCIFAR10(Dataset):\n",
        "    def __init__(self, dataset, noise_rate=0.4, asymmetric=False):\n",
        "        self.data = dataset.data\n",
        "        self.targets = np.array(dataset.targets)\n",
        "        self.transform = dataset.transform\n",
        "        self.noise_rate = noise_rate\n",
        "        self.noisy_labels = self.apply_noise(self.targets, noise_rate, asymmetric)\n",
        "\n",
        "    def apply_noise(self, labels, noise_rate, asymmetric):\n",
        "        noisy_labels = labels.copy()\n",
        "        for c in np.unique(labels):\n",
        "            class_idx = np.where(labels == c)[0]\n",
        "            noisy_idx = np.random.choice(class_idx, int(noise_rate * len(class_idx)), replace=False)\n",
        "            for i in noisy_idx:\n",
        "                if asymmetric:\n",
        "                    noisy_labels[i] = (c + 1) % 10\n",
        "                else:\n",
        "                    noisy_labels[i] = random.choice([x for x in range(10) if x != c])\n",
        "        return noisy_labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.noisy_labels[idx]\n",
        "        img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# === CNN Model ===\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "# === Evaluation ===\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    return accuracy_score(all_labels, all_preds)\n",
        "\n",
        "# === Experiment Setup ===\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=128, shuffle=False)\n",
        "\n",
        "noise_rates = [0.25, 0.5, 0.75]\n",
        "num_epochs = 20\n",
        "\n",
        "# Normalized alpha/beta for APL\n",
        "apl_weights = {\n",
        "    0.25: (0.7, 0.3),\n",
        "    0.5: (0.5, 0.5),\n",
        "    0.75: (0.3, 0.7)\n",
        "}\n",
        "\n",
        "final_accuracies = {'CE': [], 'NCE': [], 'APL': []}\n",
        "\n",
        "for noise_rate in noise_rates:\n",
        "    print(f\"\\nTraining with noise rate = {noise_rate}\")\n",
        "    alpha, beta = apl_weights[noise_rate]\n",
        "\n",
        "    trainset = NoisyCIFAR10(cifar_train, noise_rate=noise_rate, asymmetric=False)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "    loss_configs = {\n",
        "        'CE': nn.CrossEntropyLoss(),\n",
        "        'NCE': NormalizedCrossEntropy(),\n",
        "        f'APL (α={alpha}, β={beta})': ActivePassiveLoss(nn.CrossEntropyLoss(), ReverseCrossEntropy(), alpha=alpha),\n",
        "    }\n",
        "\n",
        "    epoch_logs = {}\n",
        "\n",
        "    for loss_name, loss_fn in loss_configs.items():\n",
        "        print(f\"  >> Training with {loss_name}\")\n",
        "        model = CNN().to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "        train_loss_log, train_acc_log, test_acc_log = [], [], []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "\n",
        "            with torch.enable_grad():\n",
        "                for images, labels in trainloader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(images)\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            avg_loss = total_loss / len(trainloader.dataset)\n",
        "            train_acc = evaluate(model, trainloader)\n",
        "            test_acc = evaluate(model, testloader)\n",
        "\n",
        "            train_loss_log.append(avg_loss)\n",
        "            train_acc_log.append(train_acc * 100)\n",
        "            test_acc_log.append(test_acc * 100)\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_logs[loss_name] = {\n",
        "            'train_loss': train_loss_log,\n",
        "            'train_acc': train_acc_log,\n",
        "            'test_acc': test_acc_log,\n",
        "        }\n",
        "\n",
        "        if loss_name == 'CE':\n",
        "            final_accuracies['CE'].append(test_acc_log[-1])\n",
        "        elif loss_name == 'NCE':\n",
        "            final_accuracies['NCE'].append(test_acc_log[-1])\n",
        "        else:  # It's APL\n",
        "            final_accuracies['APL'].append(test_acc_log[-1])\n",
        "\n",
        "    # === Plot per-noise-rate Loss & Accuracy ===\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for loss_name in epoch_logs:\n",
        "        plt.plot(epoch_logs[loss_name]['train_loss'], label=f\"{loss_name}\")\n",
        "    plt.title(f\"Training Loss (Noise Rate = {noise_rate})\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for loss_name in epoch_logs:\n",
        "        plt.plot(epoch_logs[loss_name]['train_acc'], label=f\"{loss_name} Train Acc\", linestyle='--')\n",
        "        plt.plot(epoch_logs[loss_name]['test_acc'], label=f\"{loss_name} Test Acc\")\n",
        "    plt.title(f\"Accuracy (Noise Rate = {noise_rate})\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.suptitle(\"Loss and Accuracy Plots\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Final Comparison Plot ===\n",
        "plt.figure(figsize=(10, 5))\n",
        "for loss_name, accs in final_accuracies.items():\n",
        "    plt.plot(noise_rates[:len(accs)], accs, marker='o', label=loss_name)\n",
        "plt.title(\"Performance Comparison Across Different Noise Rates\")\n",
        "plt.xlabel(\"Noise Rate\")\n",
        "plt.ylabel(\"Final Test Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mwVFk6wFaQK"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainset = NoisyCIFAR10(cifar_train, noise_rate=0.4, asymmetric=False)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=128, shuffle=False)\n",
        "\n",
        "losses = {\n",
        "    'CrossEntropy': nn.CrossEntropyLoss(),\n",
        "    'FocalLoss': FocalLoss(),\n",
        "    'NCE': NormalizedCrossEntropy(),\n",
        "    'NFL': NormalizedFocalLoss(),\n",
        "    'APL': ActivePassiveLoss(nn.CrossEntropyLoss(), ReverseCrossEntropy())\n",
        "}\n",
        "#run if errors:\n",
        "\n",
        "'''print(\"\\nRunning sanity check on all loss functions...\\n\")\n",
        "model = CNN().to(device)\n",
        "model.train()\n",
        "dummy_input = torch.randn(4, 3, 32, 32).to(device)\n",
        "dummy_target = torch.randint(0, 10, (4,)).to(device)\n",
        "\n",
        "for name, loss_fn in losses.items():\n",
        "    try:\n",
        "        with torch.enable_grad():\n",
        "            print(f\"Testing: {name}\")\n",
        "            output = model(dummy_input)\n",
        "            print(f\"  Output requires_grad: {output.requires_grad}, grad_fn: {output.grad_fn}\")\n",
        "            loss = loss_fn(output, dummy_target)\n",
        "            print(f\"  Loss requires_grad: {loss.requires_grad}, grad_fn: {loss.grad_fn}\")\n",
        "            loss.backward()\n",
        "            print(f\"  ✅ {name} passed.\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ {name} failed with error: {e}\\n\")'''\n",
        "\n",
        "\n",
        "results = {}\n",
        "num_epochs = 15\n",
        "loss_history = {}\n",
        "\n",
        "for name, loss_fn in losses.items():\n",
        "    model = CNN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        with torch.enable_grad():\n",
        "            for images, labels in trainloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        avg_loss = running_loss / len(trainloader.dataset)\n",
        "        acc = evaluate(model, testloader)\n",
        "        epoch_losses.append(avg_loss)\n",
        "        epoch_accuracies.append(acc)\n",
        "        scheduler.step()\n",
        "\n",
        "    results[name] = epoch_accuracies[-1]\n",
        "    loss_history[name] = (epoch_losses, epoch_accuracies)\n",
        "\n",
        "for name in loss_history:\n",
        "    losses_list, accuracies = loss_history[name]\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(accuracies, label=name)\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(losses_list, label=name)\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.suptitle(f\"Performance of {name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJywhwH6Y-nP"
      },
      "source": [
        "# **DIFFUSION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1pzPnJFTVyZ"
      },
      "source": [
        "# ***Part 1:***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPLA4QaJQQk9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rdOu91kTbzh"
      },
      "outputs": [],
      "source": [
        "image_size = 256 #@param [256, 512]\n",
        "vae_model = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
        "latent_size = int(image_size) // 8\n",
        "# Load model:\n",
        "model = DiT_XL_2(input_size=latent_size).to(device)\n",
        "state_dict = find_model(f\"DiT-XL-2-{image_size}x{image_size}.pt\")\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval() # important!\n",
        "vae = AutoencoderKL.from_pretrained(vae_model).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FX3uGbsToSN"
      },
      "outputs": [],
      "source": [
        "# Set user inputs:\n",
        "seed = 0 #@param {type:\"number\"}\n",
        "torch.manual_seed(seed)\n",
        "num_sampling_steps = 13 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "cfg_scale = 5 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
        "class_labels = [89] #@param {type:\"raw\"}\n",
        "samples_per_row = 50 #@param {type:\"number\"}\n",
        "\n",
        "# Create diffusion object:\n",
        "diffusion = create_diffusion(str(num_sampling_steps))\n",
        "\n",
        "# Create sampling noise:\n",
        "n = len(class_labels)\n",
        "z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
        "y = torch.tensor(class_labels, device=device)\n",
        "\n",
        "# Setup classifier-free guidance:\n",
        "z = torch.cat([z, z], 0)\n",
        "y_null = torch.tensor([1000] * n, device=device)\n",
        "y = torch.cat([y, y_null], 0)\n",
        "model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
        "\n",
        "# Sample images:\n",
        "samples = diffusion.p_sample_loop(\n",
        "    model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
        "    model_kwargs=model_kwargs, progress=True, device=device\n",
        ")\n",
        "samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
        "samples = vae.decode(samples / 0.18215).sample\n",
        "\n",
        "# Save and display images:\n",
        "save_image(samples, \"sample.png\", nrow=int(samples_per_row),\n",
        "           normalize=True, value_range=(-1, 1))\n",
        "samples = Image.open(\"sample.png\")\n",
        "display(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhKTE8vEbCGw"
      },
      "source": [
        "\n",
        "**At low values of cfg** : Weaker guidance, meaning the model doesn’t strongly enforce the class label.  images may be more diverse, creative, or even a bit off-label. *underfitting-looking results*\n",
        "\n",
        "\n",
        "**At high values of cfg** : Very strong guidance: the model strictly follows the class conditioning. *overfitting-looking results*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5yPaFWxbplD"
      },
      "source": [
        "**At Low Sample Steps :** Image appear blurry due to noise not being removed entirely due to low number of denoising steps. Output is fast\n",
        "\n",
        "**At High Sample Steps :** Image appear Very detailed and crisp due to noise being completely removed. Output is very slow and there is diminished returns past a point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ookp3XhujGQv"
      },
      "source": [
        "# ***Part 2:***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03beP1qhTPUO"
      },
      "source": [
        "# i:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LHJhYJduE7U"
      },
      "outputs": [],
      "source": [
        "#Important Libraries:\n",
        "!pip install xformers --upgrade\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from timm.models.vision_transformer import PatchEmbed, Mlp\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from dataclasses import asdict, dataclass\n",
        "from typing import Optional, Type, TypeVar\n",
        "\n",
        "from xformers._deprecation_warning import deprecated_function\n",
        "from xformers.components.attention import AttentionMask\n",
        "from xformers.components.attention import build_attention\n",
        "from xformers.ops import memory_efficient_attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUx9brtAcrfT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Efficient Attention ===\n",
        "try:\n",
        "    from flash_attn import flash_attn_func\n",
        "    def memory_efficient_attention(q, k, v):\n",
        "        return flash_attn_func(q, k, v, dropout_p=0.0, causal=False)\n",
        "except ImportError:\n",
        "    try:\n",
        "        import xformers.ops\n",
        "        def memory_efficient_attention(q, k, v):\n",
        "            return xformers.ops.memory_efficient_attention(q, k, v)\n",
        "    except ImportError:\n",
        "        def memory_efficient_attention(q, k, v):\n",
        "            attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))\n",
        "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "            return torch.matmul(attn_probs, v)\n",
        "\n",
        "# === Modulated LayerNorm ===\n",
        "def modulated_layernorm(x, shift, scale, eps=1e-6):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    var = x.var(-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + eps)\n",
        "    return norm_x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "# === Fused MLP ===\n",
        "class FusedMlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.SiLU()\n",
        "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.act(self.fc1(x)))\n",
        "\n",
        "# === Optimized DiT Block ===\n",
        "class FastDiTBlock(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        self.qkv = nn.Linear(hidden_size, hidden_size * 3, bias=False)\n",
        "        self.mlp = FusedMlp(hidden_size, int(hidden_size * mlp_ratio))\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 6 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        B, N, D = x.shape\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
        "\n",
        "        x_msa = modulated_layernorm(x, shift_msa, scale_msa)\n",
        "        qkv = self.qkv(x_msa).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn_out = memory_efficient_attention(q, k, v).transpose(1, 2).reshape(B, N, D)\n",
        "\n",
        "        x = x + gate_msa.unsqueeze(1) * attn_out\n",
        "        x_mlp = modulated_layernorm(x, shift_mlp, scale_mlp)\n",
        "        x = x + gate_mlp.unsqueeze(1) * self.mlp(x_mlp)\n",
        "\n",
        "        return x\n",
        "\n",
        "# === Dummy Model with DiT Blocks ===\n",
        "class DummyDiT(nn.Module):\n",
        "    def __init__(self, hidden_size=768, num_heads=12, depth=12):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([FastDiTBlock(hidden_size, num_heads) for _ in range(depth)])\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)\n",
        "        return x\n",
        "\n",
        "# === Benchmarking ===\n",
        "def benchmark_model(model, batch_sizes, num_tokens=256, hidden_size=768, steps=50):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "    times = []\n",
        "    for bs in batch_sizes:\n",
        "        x = torch.randn(bs, num_tokens, hidden_size, device=device)\n",
        "        c = torch.randn(bs, hidden_size, device=device)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(steps):\n",
        "                _ = model(x, c)\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"Batch size {bs}: {elapsed:.2f}s for {steps} steps\")\n",
        "        times.append(elapsed)\n",
        "    return times\n",
        "\n",
        "# === Plotting ===\n",
        "def plot_batch_timing(batch_sizes, times, title=\"Sampling Time vs Batch Size\"):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(batch_sizes, times, marker='o')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Batch Size\")\n",
        "    plt.ylabel(\"Time (s) for 50 Iterations\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Run It ===\n",
        "batch_sizes = [50, 100, 150, 200, 250]\n",
        "model = DummyDiT()\n",
        "times = benchmark_model(model, batch_sizes)\n",
        "plot_batch_timing(batch_sizes, times)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kFjfapsljoX"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "# --------------------------------------------------------\n",
        "# References:\n",
        "# GLIDE: https://github.com/openai/glide-text2im\n",
        "# MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
        "\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#               Embedding Layers for Timesteps and Class Labels                 #\n",
        "#################################################################################\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds scalar timesteps into vector representations.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
        "        )\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    @staticmethod\n",
        "    def timestep_embedding(t, dim, max_period=10000):\n",
        "        \"\"\"\n",
        "        Create sinusoidal timestep embeddings.\n",
        "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
        "                          These may be fractional.\n",
        "        :param dim: the dimension of the output.\n",
        "        :param max_period: controls the minimum frequency of the embeddings.\n",
        "        :return: an (N, D) Tensor of positional embeddings.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(\n",
        "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "        ).to(device=t.device)\n",
        "        args = t[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, t):\n",
        "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class LabelEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, hidden_size, dropout_prob):\n",
        "        super().__init__()\n",
        "        use_cfg_embedding = dropout_prob > 0\n",
        "        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def token_drop(self, labels, force_drop_ids=None):\n",
        "        \"\"\"\n",
        "        Drops labels to enable classifier-free guidance.\n",
        "        \"\"\"\n",
        "        if force_drop_ids is None:\n",
        "            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n",
        "        else:\n",
        "            drop_ids = force_drop_ids == 1\n",
        "        labels = torch.where(drop_ids, self.num_classes, labels)\n",
        "        return labels\n",
        "\n",
        "    def forward(self, labels, train, force_drop_ids=None):\n",
        "        use_dropout = self.dropout_prob > 0\n",
        "        if (train and use_dropout) or (force_drop_ids is not None):\n",
        "            labels = self.token_drop(labels, force_drop_ids)\n",
        "        embeddings = self.embedding_table(labels)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                 Core DiT Model                                #\n",
        "#################################################################################\n",
        "\n",
        "class DiTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
        "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
        "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 6 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
        "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    The final layer of DiT.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, patch_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DiT(nn.Module):\n",
        "    \"\"\"\n",
        "    Diffusion model with a Transformer backbone.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=32,\n",
        "        patch_size=2,\n",
        "        in_channels=4,\n",
        "        hidden_size=1152,\n",
        "        depth=28,\n",
        "        num_heads=16,\n",
        "        mlp_ratio=4.0,\n",
        "        class_dropout_prob=0.1,\n",
        "        num_classes=1000,\n",
        "        learn_sigma=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.learn_sigma = learn_sigma\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
        "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        # Will use fixed sin-cos embedding:\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
        "        ])\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.proj.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
        "\n",
        "        # Initialize label embedding table:\n",
        "        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\n",
        "\n",
        "        # Initialize timestep embedding MLP:\n",
        "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size**2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels\n",
        "        p = self.x_embedder.patch_size[0]\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def forward(self, x, t, y):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT.\n",
        "        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\n",
        "        t: (N,) tensor of diffusion timesteps\n",
        "        y: (N,) tensor of class labels\n",
        "        \"\"\"\n",
        "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)                   # (N, D)\n",
        "        y = self.y_embedder(y, self.training)    # (N, D)\n",
        "        c = t + y                                # (N, D)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)                      # (N, T, D)\n",
        "        x = self.final_layer(x, c)                # (N, T, patch_size ** 2 * out_channels)\n",
        "        x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
        "        return x\n",
        "\n",
        "    def forward_with_cfg(self, x, t, y, cfg_scale):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
        "        half = x[: len(x) // 2]\n",
        "        combined = torch.cat([half, half], dim=0)\n",
        "        model_out = self.forward(combined, t, y)\n",
        "        # For exact reproducibility reasons, we apply classifier-free guidance on only\n",
        "        # three channels by default. The standard approach to cfg applies it to all channels.\n",
        "        # This can be done by uncommenting the following line and commenting-out the line following that.\n",
        "        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
        "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
        "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "        return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                   Sine/Cosine Positional Embedding Functions                  #\n",
        "#################################################################################\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token and extra_tokens > 0:\n",
        "        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                   DiT Configs                                  #\n",
        "#################################################################################\n",
        "\n",
        "def DiT_XL_2(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=2, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_XL_4(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=4, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_XL_8(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=8, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_2(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=2, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_4(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=4, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_8(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=8, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_B_2(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=2, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_B_4(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=4, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_B_8(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=8, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_S_2(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6, **kwargs)\n",
        "\n",
        "def DiT_S_4(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=4, num_heads=6, **kwargs)\n",
        "\n",
        "def DiT_S_8(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=8, num_heads=6, **kwargs)\n",
        "\n",
        "\n",
        "DiT_models = {\n",
        "    'DiT-XL/2': DiT_XL_2,  'DiT-XL/4': DiT_XL_4,  'DiT-XL/8': DiT_XL_8,\n",
        "    'DiT-L/2':  DiT_L_2,   'DiT-L/4':  DiT_L_4,   'DiT-L/8':  DiT_L_8,\n",
        "    'DiT-B/2':  DiT_B_2,   'DiT-B/4':  DiT_B_4,   'DiT-B/8':  DiT_B_8,\n",
        "    'DiT-S/2':  DiT_S_2,   'DiT-S/4':  DiT_S_4,   'DiT-S/8':  DiT_S_8,\n",
        "}\n",
        "\n",
        "def plot_batch_timing(batch_sizes, times, title=\"Sampling Time vs Batch Size\"):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(batch_sizes, times, marker='o', linestyle='-')\n",
        "    plt.xlabel(\"Batch Size\")\n",
        "    plt.ylabel(\"Time to Sample 50 Images (s)\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.xticks(batch_sizes)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "batch_sizes = [50, 100, 150, 200, 250]\n",
        "times = []\n",
        "\n",
        "print(type(model.blocks[0].attn))\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    x = torch.randn(bs, 4, 32, 32).to(device)\n",
        "    t = torch.randint(0, 1000, (bs,), device=device)\n",
        "    y = torch.randint(0, 1000, (bs,), device=device)\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(50):\n",
        "            _ = model(x, t, y)\n",
        "    end = time.time()\n",
        "\n",
        "    elapsed = end - start\n",
        "    print(f\" Sampled 50 images with batch size {bs} in {elapsed:.2f} seconds\")\n",
        "    times.append(elapsed)\n",
        "\n",
        "plot_batch_timing(batch_sizes, times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSVEkzhocxzl"
      },
      "source": [
        "**Fix 1:** Added Memory effecient attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfpwvRyXS2jw"
      },
      "source": [
        "# ii:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ftwM9gmzOpN4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content\"):\n",
        "    if \"models.py\" in files:\n",
        "        print(\"Found models.py at:\", os.path.join(root, \"models.py\"))\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"arnaud58/landscape-pictures\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "!pip install torch torchvision timm accelerate einops scipy ftfy\n",
        "!pip install torchmetrics\n",
        "!git clone https://github.com/facebookresearch/DiT.git\n",
        "%cd DiT\n",
        "\n",
        "!pip install torch torchvision timm accelerate einops scipy ftfy torchmetrics\n",
        "\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from models import DiT_B_8\n",
        "\n",
        "#loading data set\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "\n",
        "new_root = '/content/landscape_dataset/images'\n",
        "\n",
        "\n",
        "os.makedirs(new_root, exist_ok=True)\n",
        "\n",
        "for file in os.listdir(path):\n",
        "    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        src = os.path.join(path, file)\n",
        "        dst = os.path.join(new_root, file)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "dataset_root = '/content/landscape_dataset'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3) ])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=dataset_root, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "real_img, _ = next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P9P_8KOuHlC"
      },
      "outputs": [],
      "source": [
        "#cloning DiT repo\n",
        "\n",
        "\n",
        "!pip install torch torchvision timm accelerate einops scipy ftfy\n",
        "!pip install torchmetrics\n",
        "!git clone https://github.com/facebookresearch/DiT.git\n",
        "%cd DiT\n",
        "\n",
        "!pip install torch torchvision timm accelerate einops scipy ftfy torchmetrics\n",
        "\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from models import DiT_B_8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lEBCFpZ9gc4"
      },
      "outputs": [],
      "source": [
        "#loading data set\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "\n",
        "new_root = '/content/landscape_dataset/images'\n",
        "\n",
        "\n",
        "os.makedirs(new_root, exist_ok=True)\n",
        "\n",
        "for file in os.listdir(path):\n",
        "    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        src = os.path.join(path, file)\n",
        "        dst = os.path.join(new_root, file)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "dataset_root = '/content/landscape_dataset'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3) ])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=dataset_root, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "real_img, _ = next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f3cFre4wKW7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "from torch.optim.swa_utils import AveragedModel, update_bn\n",
        "\n",
        "\n",
        "new_root = '/content/landscape_dataset/images'\n",
        "\n",
        "\n",
        "os.makedirs(new_root, exist_ok=True)\n",
        "\n",
        "for file in os.listdir(path):\n",
        "    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        src = os.path.join(path, file)\n",
        "        dst = os.path.join(new_root, file)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "dataset_root = '/content/landscape_dataset'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3) ])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=dataset_root, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "real_img, _ = next(iter(dataloader))\n",
        "fake_channel = torch.zeros((real_img.shape[0], 1, 256, 256))\n",
        "real_img_4ch = torch.cat([real_img, fake_channel], dim=1)\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds scalar timesteps into vector representations.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
        "        )\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    @staticmethod\n",
        "    def timestep_embedding(t, dim, max_period=10000):\n",
        "        \"\"\"\n",
        "        Create sinusoidal timestep embeddings.\n",
        "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
        "                          These may be fractional.\n",
        "        :param dim: the dimension of the output.\n",
        "        :param max_period: controls the minimum frequency of the embeddings.\n",
        "        :return: an (N, D) Tensor of positional embeddings.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(\n",
        "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "        ).to(device=t.device)\n",
        "        args = t[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, t):\n",
        "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class LabelEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, hidden_size, dropout_prob):\n",
        "        super().__init__()\n",
        "        use_cfg_embedding = dropout_prob > 0\n",
        "        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def token_drop(self, labels, force_drop_ids=None):\n",
        "        \"\"\"\n",
        "        Drops labels to enable classifier-free guidance.\n",
        "        \"\"\"\n",
        "        if force_drop_ids is None:\n",
        "            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n",
        "        else:\n",
        "            drop_ids = force_drop_ids == 1\n",
        "        labels = torch.where(drop_ids, self.num_classes, labels)\n",
        "        return labels\n",
        "\n",
        "    def forward(self, labels, train, force_drop_ids=None):\n",
        "        use_dropout = self.dropout_prob > 0\n",
        "        if (train and use_dropout) or (force_drop_ids is not None):\n",
        "            labels = self.token_drop(labels, force_drop_ids)\n",
        "        embeddings = self.embedding_table(labels)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                 Core DiT Model                                #\n",
        "#################################################################################\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, window_size=5, qkv_bias=True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, dim)\n",
        "\n",
        "        attn_scores = torch.zeros(B, self.num_heads, N, N, device=x.device)\n",
        "\n",
        "        for i in range(N):\n",
        "            start = max(0, i - self.window_size)\n",
        "            end = min(N, i + self.window_size + 1)\n",
        "            qi = q[:, :, i:i+1, :]  # (B, heads, 1, dim)\n",
        "            ki = k[:, :, start:end, :]  # (B, heads, W, dim)\n",
        "            attn_scores[:, :, i:i+1, start:end] = (qi @ ki.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        attn = attn_scores.softmax(dim=-1)\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).reshape(B, N, C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class DiTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        if block_kwargs.get(\"use_swa\", False):\n",
        "          self.attn = SlidingWindowAttention(hidden_size, num_heads=num_heads, window_size=block_kwargs.get(\"window_size\", 5))\n",
        "        else:\n",
        "          self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
        "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
        "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 6 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
        "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    The final layer of DiT.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, patch_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DiT(nn.Module):\n",
        "    \"\"\"\n",
        "    Diffusion model with a Transformer backbone.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=32,\n",
        "        patch_size=2,\n",
        "        in_channels=4,\n",
        "        hidden_size=1152,\n",
        "        depth=28,\n",
        "        num_heads=16,\n",
        "        mlp_ratio=4.0,\n",
        "        class_dropout_prob=0.1,\n",
        "        num_classes=1000,\n",
        "        learn_sigma=True,\n",
        "        use_swa=False,\n",
        "        window_size=5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.learn_sigma = learn_sigma\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
        "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        # Will use fixed sin-cos embedding:\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio,use_swa=use_swa, window_size=window_size) for _ in range(depth)\n",
        "        ])\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.proj.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
        "\n",
        "        # Initialize label embedding table:\n",
        "        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\n",
        "\n",
        "        # Initialize timestep embedding MLP:\n",
        "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size**2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels\n",
        "        p = self.x_embedder.patch_size[0]\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def forward(self, x, t, y):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT.\n",
        "        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\n",
        "        t: (N,) tensor of diffusion timesteps\n",
        "        y: (N,) tensor of class labels\n",
        "        \"\"\"\n",
        "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D)  T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)                   # (N, D)\n",
        "        y = torch.zeros_like(t)\n",
        "    # (N, D)\n",
        "        c = t + y                                # (N, D)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)                      # (N, T, D)\n",
        "        x = self.final_layer(x, c)                # (N, T, patch_size ** 2 * out_channels)\n",
        "        x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
        "        return x\n",
        "\n",
        "    def forward_with_cfg(self, x, t, y, cfg_scale):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
        "        half = x[: len(x) // 2]\n",
        "        combined = torch.cat([half, half], dim=0)\n",
        "        model_out = self.forward(combined, t, y)\n",
        "        # For exact reproducibility reasons, we apply classifier-free guidance on only\n",
        "        # three channels by default. The standard approach to cfg applies it to all channels.\n",
        "        # This can be done by uncommenting the following line and commenting-out the line following that.\n",
        "        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
        "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
        "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "        return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                   Sine/Cosine Positional Embedding Functions                  #\n",
        "#################################################################################\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token and extra_tokens > 0:\n",
        "        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "def plot_output_comparison(out_full, out_swa, title1=\"Full Attention\", title2=\"SWA\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from torchvision.utils import make_grid\n",
        "\n",
        "    def img_tensor_to_numpy(img):\n",
        "        img = (img.clamp(-1, 1) + 1) / 2\n",
        "        img = make_grid(img[:, :3], nrow=1).permute(1, 2, 0).cpu().detach().numpy()\n",
        "        return img\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].imshow(img_tensor_to_numpy(out_full))\n",
        "    axs[0].set_title(title1)\n",
        "    axs[0].axis(\"off\")\n",
        "    axs[1].imshow(img_tensor_to_numpy(out_swa))\n",
        "    axs[1].set_title(title2)\n",
        "    axs[1].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "#################################################################################\n",
        "#                                   DiT Configs                                  #\n",
        "#################################################################################\n",
        "\n",
        "def DiT_XL_2(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=2, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_XL_4(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=4, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_XL_8(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=8, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_2(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=2, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_4(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=4, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_8(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=8, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_B_2(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=2, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_B_4(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=4, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_B_8(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=8, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_S_2(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6, **kwargs)\n",
        "\n",
        "def DiT_S_4(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=4, num_heads=6, **kwargs)\n",
        "\n",
        "def DiT_S_8(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=8, num_heads=6, **kwargs)\n",
        "\n",
        "\n",
        "DiT_models = {\n",
        "    'DiT-XL/2': DiT_XL_2,  'DiT-XL/4': DiT_XL_4,  'DiT-XL/8': DiT_XL_8,\n",
        "    'DiT-L/2':  DiT_L_2,   'DiT-L/4':  DiT_L_4,   'DiT-L/8':  DiT_L_8,\n",
        "    'DiT-B/2':  DiT_B_2,   'DiT-B/4':  DiT_B_4,   'DiT-B/8':  DiT_B_8,\n",
        "    'DiT-S/2':  DiT_S_2,   'DiT-S/4':  DiT_S_4,   'DiT-S/8':  DiT_S_8,\n",
        "}\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "model_full = DiT_B_8(input_size=256, in_channels=4).to(device)\n",
        "model_swa = AveragedModel(model_full).to(device)\n",
        "\n",
        "real_img_4ch = real_img_4ch.to(device)\n",
        "t = torch.tensor([1], dtype=torch.long, device=device)\n",
        "y = torch.tensor([0], dtype=torch.long, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_full = model_full(real_img_4ch, t, y)\n",
        "    out_swa  = model_swa(real_img_4ch, t, y)\n",
        "\n",
        "real_img_4ch = real_img_4ch.to(device)\n",
        "t = t.to(device)\n",
        "y = y.to(device)\n",
        "model_full = model_full.to(device)\n",
        "model_swa = model_swa.to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer_full = optim.Adam(model_full.parameters(), lr=1e-4)\n",
        "optimizer_swa  = optim.Adam(model_swa.parameters(),  lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHoFysMKaNN_"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics[image]\n",
        "\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS7rGH69Qndu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "fid_full = FrechetInceptionDistance(feature=2048).to(device)\n",
        "fid_swa = FrechetInceptionDistance(feature=2048).to(device)\n",
        "\n",
        "def denormalize_to_uint8(tensor):\n",
        "    tensor = (tensor * 0.5 + 0.5).clamp(0, 1)\n",
        "    return (tensor * 255).to(torch.uint8)\n",
        "\n",
        "fid_scores_full = []\n",
        "fid_scores_swa = []\n",
        "\n",
        "num_epochs = 20\n",
        "swa_start = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_full.train()\n",
        "\n",
        "    for i, (real_img, _) in enumerate(dataloader):\n",
        "        if i >= 50: break\n",
        "\n",
        "        real_img = real_img.to(device)\n",
        "        fake_channel = torch.zeros((real_img.size(0), 1, 256, 256), device=device)\n",
        "        real_img_4ch = torch.cat([real_img, fake_channel], dim=1)\n",
        "\n",
        "        t = torch.randint(0, 1000, (real_img.size(0),), device=device)\n",
        "        y = torch.zeros(real_img.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "        optimizer_full.zero_grad()\n",
        "        out_full = model_full(real_img_4ch, t, y)\n",
        "        loss_full = loss_fn(out_full[:, :3], real_img)\n",
        "        loss_full.backward()\n",
        "        optimizer_full.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Step {i}, Full Loss: {loss_full.item():.4f}\")\n",
        "\n",
        "\n",
        "    if epoch >= swa_start:\n",
        "        model_swa.update_parameters(model_full)\n",
        "\n",
        "\n",
        "    model_full.eval()\n",
        "    model_swa.eval()\n",
        "    fid_full.reset()\n",
        "    fid_swa.reset()\n",
        "\n",
        "    if epoch >= swa_start:\n",
        "        update_bn(dataloader, model_swa)\n",
        "\n",
        "    real_imgs = []\n",
        "    for real_img, _ in dataloader:\n",
        "        real_img = real_img.to(device)\n",
        "        real_imgs.append(real_img)\n",
        "        real_img_uint8 = denormalize_to_uint8(real_img).to(device)\n",
        "        fid_full.update(real_img_uint8, real=True)\n",
        "        if epoch >= swa_start:\n",
        "            fid_swa.update(real_img_uint8, real=True)\n",
        "        if len(real_imgs) >= 50:\n",
        "            break\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img in real_imgs:\n",
        "            B = img.shape[0]\n",
        "            z = torch.zeros((B, 1, 256, 256), device=device)\n",
        "            x = torch.cat([img, z], dim=1)\n",
        "            t = torch.randint(0, 1000, (B,), device=device)\n",
        "            y = torch.zeros(B, dtype=torch.long, device=device)\n",
        "\n",
        "            fake_full = model_full(x, t, y)\n",
        "            fake_full_uint8 = denormalize_to_uint8(fake_full[:, :3]).to(device)\n",
        "            fid_full.update(fake_full_uint8, real=False)\n",
        "\n",
        "            if epoch >= swa_start:\n",
        "                fake_swa = model_swa(x, t, y)\n",
        "                fake_swa_uint8 = denormalize_to_uint8(fake_swa[:, :3]).to(device)\n",
        "                fid_swa.update(fake_swa_uint8, real=False)\n",
        "\n",
        "    score_full = fid_full.compute().item()\n",
        "    score_swa = fid_swa.compute().item() if epoch >= swa_start else float('nan')\n",
        "\n",
        "    fid_scores_full.append(score_full)\n",
        "    fid_scores_swa.append(score_swa)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}: FID (Full) = {score_full:.2f}\", end='')\n",
        "    if epoch >= swa_start:\n",
        "        print(f\", FID (SWA) = {score_swa:.2f}\\n\")\n",
        "    else:\n",
        "        print(\", FID (SWA) = N/A (not started)\\n\")\n",
        "\n",
        "\n",
        "    if epoch >= swa_start:\n",
        "        plot_output_comparison(fake_full, fake_swa)\n",
        "    else:\n",
        "        plot_output_comparison(fake_full, fake_full)\n",
        "\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, fid_scores_full, label='Full Attention', marker='o')\n",
        "plt.plot(epochs, fid_scores_swa, label='SWA', marker='s')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"FID Score (lower is better)\")\n",
        "plt.title(\"FID Score vs Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyKxD1UREPiT"
      },
      "source": [
        "# ***Part 3:***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFm2aLUER5zE"
      },
      "source": [
        "# i:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_nDC6x6CWxl"
      },
      "source": [
        "*All Diffusion Part 2 Cells must be run first*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjMIVTGaBiIc"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "def get_clip_features(images):\n",
        "    image_inputs = torch.stack([preprocess(img) for img in images]).to(device)\n",
        "    with torch.no_grad():\n",
        "        features = model_clip.encode_image(image_inputs)\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "    return features\n",
        "\n",
        "def gaussian_kernel(x, y, sigma=1.0):\n",
        "    x = x.unsqueeze(1)\n",
        "    y = y.unsqueeze(0)\n",
        "    dist = ((x - y) ** 2).sum(-1)\n",
        "    return torch.exp(-dist / (2 * sigma ** 2))\n",
        "\n",
        "def compute_mmd(x, y, sigma=1.0):\n",
        "    K_xx = gaussian_kernel(x, x, sigma).mean()\n",
        "    K_yy = gaussian_kernel(y, y, sigma).mean()\n",
        "    K_xy = gaussian_kernel(x, y, sigma).mean()\n",
        "    return K_xx + K_yy - 2 * K_xy\n",
        "\n",
        "def compute_cmmd(real_images, gen_images, sigma=1.0):\n",
        "    real_feats = get_clip_features(real_images)\n",
        "    gen_feats = get_clip_features(gen_images)\n",
        "    mmd_score = compute_mmd(real_feats, gen_feats, sigma)\n",
        "    return mmd_score.item()\n",
        "\n",
        "def denormalize_to_uint8(tensor):\n",
        "    tensor = (tensor * 0.5 + 0.5).clamp(0, 1)\n",
        "    return (tensor * 255).to(torch.uint8)\n",
        "\n",
        "def smooth_curve(values, window=3):\n",
        "    return np.convolve(values, np.ones(window)/window, mode='valid')\n",
        "\n",
        "\n",
        "def train_model(model_full, model_swa, dataloader, update_bn, num_epochs=100, swa_start=10):\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    optimizer_full = torch.optim.Adam(model_full.parameters(), lr=1e-4)\n",
        "    optimizer_swa = torch.optim.Adam(model_swa.parameters(), lr=1e-4)\n",
        "    to_pil = ToPILImage()\n",
        "\n",
        "    fid_full = FrechetInceptionDistance(feature=2048).to(device)\n",
        "    fid_swa = FrechetInceptionDistance(feature=2048).to(device)\n",
        "\n",
        "    fid_scores_full = []\n",
        "    fid_scores_swa = []\n",
        "    cmmd_full_scores = []\n",
        "    cmmd_swa_scores = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model_full.train()\n",
        "        for i, (real_img, _) in enumerate(dataloader):\n",
        "            if i >= 50: break\n",
        "            real_img = real_img.to(device)\n",
        "            fake_channel = torch.zeros((real_img.size(0), 1, 256, 256), device=device)\n",
        "            real_img_4ch = torch.cat([real_img, fake_channel], dim=1)\n",
        "\n",
        "            t = torch.randint(0, 1000, (real_img.size(0),), device=device)\n",
        "            y = torch.zeros(real_img.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "            optimizer_full.zero_grad()\n",
        "            out_full = model_full(real_img_4ch, t, y)\n",
        "            loss_full = loss_fn(out_full[:, :3], real_img)\n",
        "            loss_full.backward()\n",
        "            optimizer_full.step()\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Step {i}, Full Loss: {loss_full.item():.4f}\")\n",
        "\n",
        "        if epoch >= swa_start:\n",
        "            model_swa.update_parameters(model_full)\n",
        "\n",
        "        model_full.eval()\n",
        "        model_swa.eval()\n",
        "        fid_full.reset()\n",
        "        fid_swa.reset()\n",
        "\n",
        "        if epoch >= swa_start:\n",
        "            update_bn(dataloader, model_swa)\n",
        "\n",
        "        real_images = []\n",
        "        for real_img, _ in dataloader:\n",
        "            real_img = real_img.to(device)\n",
        "            real_imgs_uint8 = denormalize_to_uint8(real_img).to(device)\n",
        "            fid_full.update(real_imgs_uint8, real=True)\n",
        "            if epoch >= swa_start:\n",
        "                fid_swa.update(real_imgs_uint8, real=True)\n",
        "\n",
        "            for i in range(real_img.size(0)):\n",
        "                real_images.append(to_pil(real_img[i].cpu().clamp(-1, 1)))\n",
        "\n",
        "            if len(real_images) >= 100:\n",
        "                break\n",
        "\n",
        "        fake_full_pil = []\n",
        "        fake_swa_pil = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10):\n",
        "                real_img_batch, _ = next(iter(dataloader))\n",
        "                real_img_batch = real_img_batch.to(device)\n",
        "                z = torch.zeros((real_img_batch.size(0), 1, 256, 256), device=device)\n",
        "                x = torch.cat([real_img_batch, z], dim=1)\n",
        "                t = torch.randint(0, 1000, (real_img_batch.size(0),), device=device)\n",
        "                y = torch.zeros(real_img_batch.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "                fake_full = model_full(x, t, y)\n",
        "                fake_full_uint8 = denormalize_to_uint8(fake_full[:, :3])\n",
        "                fid_full.update(fake_full_uint8, real=False)\n",
        "\n",
        "                for k in range(real_img_batch.size(0)):\n",
        "                    fake_full_pil.append(to_pil(fake_full[k, :3].cpu().clamp(-1, 1)))\n",
        "\n",
        "                if epoch >= swa_start:\n",
        "                    fake_swa = model_swa(x, t, y)\n",
        "                    fake_swa_uint8 = denormalize_to_uint8(fake_swa[:, :3])\n",
        "                    fid_swa.update(fake_swa_uint8, real=False)\n",
        "\n",
        "                    for k in range(real_img_batch.size(0)):\n",
        "                        fake_swa_pil.append(to_pil(fake_swa[k, :3].cpu().clamp(-1, 1)))\n",
        "\n",
        "        score_full = fid_full.compute().item()\n",
        "        score_swa = fid_swa.compute().item() if epoch >= swa_start else float('nan')\n",
        "        fid_scores_full.append(score_full)\n",
        "        fid_scores_swa.append(score_swa)\n",
        "\n",
        "        cmmd_full = compute_cmmd(real_images, fake_full_pil)\n",
        "        cmmd_swa = compute_cmmd(real_images, fake_swa_pil) if epoch >= swa_start else float('nan')\n",
        "        cmmd_full_scores.append(cmmd_full)\n",
        "        cmmd_swa_scores.append(cmmd_swa)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}: FID (Full) = {score_full:.2f}, FID (SWA) = {score_swa:.2f}\")\n",
        "        print(f\"            CMMD (Full) = {cmmd_full:.4f}, CMMD (SWA) = {cmmd_swa:.4f}\\n\")\n",
        "\n",
        "\n",
        "    epochs = list(range(1, num_epochs + 1))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, fid_scores_full, label='FID Full Attention', marker='o')\n",
        "    plt.plot(epochs, fid_scores_swa, label='FID SWA', marker='s')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"FID Score\")\n",
        "    plt.title(\"FID vs Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, cmmd_full_scores, label='CMMD Full Attention', marker='o')\n",
        "    plt.plot(epochs, cmmd_swa_scores, label='CMMD SWA', marker='s')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"CMMD Score\")\n",
        "    plt.title(\"CMMD vs Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "train_model(model_full, model_swa, dataloader, update_bn, num_epochs=20, swa_start=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5f_UJbQSBtf"
      },
      "source": [
        "# ii:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kPh4DSg4ctu"
      },
      "source": [
        "**SigLIP & ALIGN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWnEhHbTcoba"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics[image]\n",
        "!pip install torch-fidelity\n",
        "!pip install transformers timm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torch.optim.swa_utils import AveragedModel, update_bn\n",
        "import clip\n",
        "from transformers import SiglipProcessor, SiglipVisionModel, AutoProcessor, AutoModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TGAQ-RBdG6f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import ToPILImage, Resize\n",
        "import clip\n",
        "from transformers import SiglipProcessor, SiglipVisionModel, AutoProcessor, AutoModel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Image conversion\n",
        "to_pil = ToPILImage()\n",
        "resize_224 = Resize((224, 224))\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load models\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "siglip_model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\").to(device)\n",
        "siglip_processor = SiglipProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "align_model = AutoModel.from_pretrained(\"kakaobrain/align-base\").vision_model.to(device)\n",
        "align_processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n",
        "\n",
        "# Feature extraction\n",
        "def get_clip_features(images):\n",
        "    image_inputs = torch.stack([clip_preprocess(img) for img in images]).to(device)\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.encode_image(image_inputs)\n",
        "        return features / features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "def get_semantic_features(images, model, processor):\n",
        "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(pixel_values)\n",
        "        feats = outputs.last_hidden_state[:, 0, :]\n",
        "        return feats / feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# CMMD tools\n",
        "def gaussian_kernel(x, y, sigma=1.0):\n",
        "    x = x.unsqueeze(1)\n",
        "    y = y.unsqueeze(0)\n",
        "    dist = ((x - y) ** 2).sum(-1)\n",
        "    return torch.exp(-dist / (2 * sigma ** 2))\n",
        "\n",
        "def compute_mmd(x, y, sigma=1.0):\n",
        "    return gaussian_kernel(x, x, sigma).mean() + \\\n",
        "           gaussian_kernel(y, y, sigma).mean() - \\\n",
        "           2 * gaussian_kernel(x, y, sigma).mean()\n",
        "\n",
        "def compute_cmmd(real_images, gen_images, model=None, processor=None, is_clip=False):\n",
        "    if is_clip:\n",
        "        real_feats = get_clip_features(real_images)\n",
        "        gen_feats = get_clip_features(gen_images)\n",
        "    else:\n",
        "        real_feats = get_semantic_features(real_images, model, processor)\n",
        "        gen_feats = get_semantic_features(gen_images, model, processor)\n",
        "    return compute_mmd(real_feats, gen_feats).item()\n",
        "\n",
        "def smooth(data, weight=0.85):\n",
        "    smoothed = []\n",
        "    last = data[0]\n",
        "    for point in data:\n",
        "        smoothed_val = last * weight + (1 - weight) * point\n",
        "        smoothed.append(smoothed_val)\n",
        "        last = smoothed_val\n",
        "    return smoothed\n",
        "\n",
        "# Trackers\n",
        "clip_cmmd_full, clip_cmmd_swa = [], []\n",
        "siglip_cmmd_full, siglip_cmmd_swa = [], []\n",
        "align_cmmd_full, align_cmmd_swa = [], []\n",
        "\n",
        "num_epochs = 20\n",
        "swa_start = 4\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer_full = torch.optim.Adam(model_full.parameters(), lr=1e-4)\n",
        "optimizer_swa = torch.optim.Adam(model_swa.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model_full.train()\n",
        "    for i, (real_img, _) in enumerate(dataloader):\n",
        "        if i >= 50: break\n",
        "        real_img = real_img.to(device)\n",
        "        fake_channel = torch.zeros((real_img.size(0), 1, 256, 256), device=device)\n",
        "        real_img_4ch = torch.cat([real_img, fake_channel], dim=1)\n",
        "        t = torch.randint(0, 1000, (real_img.size(0),), device=device)\n",
        "        y = torch.zeros(real_img.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "        optimizer_full.zero_grad()\n",
        "        out_full = model_full(real_img_4ch, t, y)\n",
        "        loss_full = loss_fn(out_full[:, :3], real_img)\n",
        "        loss_full.backward()\n",
        "        optimizer_full.step()\n",
        "\n",
        "    # SWA update\n",
        "    if epoch >= swa_start:\n",
        "        model_swa.update_parameters(model_full)\n",
        "        update_bn(dataloader, model_swa)\n",
        "\n",
        "    # Generate real/fake samples\n",
        "    model_full.eval()\n",
        "    model_swa.eval()\n",
        "\n",
        "    real_images = []\n",
        "    for real_img, _ in dataloader:\n",
        "        real_img = real_img.to(device)\n",
        "        for i in range(real_img.size(0)):\n",
        "            real_images.append(to_pil(real_img[i].cpu().clamp(-1, 1)))\n",
        "        if len(real_images) >= 100:\n",
        "            break\n",
        "\n",
        "    fake_full_pil, fake_swa_pil = [], []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            real_img_batch, _ = next(iter(dataloader))\n",
        "            real_img_batch = real_img_batch.to(device)\n",
        "            z = torch.zeros((real_img_batch.size(0), 1, 256, 256), device=device)\n",
        "            x = torch.cat([real_img_batch, z], dim=1)\n",
        "            t = torch.randint(0, 1000, (real_img_batch.size(0),), device=device)\n",
        "            y = torch.zeros(real_img_batch.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "            fake_full = model_full(x, t, y)\n",
        "            for k in range(fake_full.size(0)):\n",
        "                fake_full_pil.append(to_pil(fake_full[k, :3].cpu().clamp(-1, 1)))\n",
        "\n",
        "            if epoch >= swa_start:\n",
        "                fake_swa = model_swa(x, t, y)\n",
        "                for k in range(fake_swa.size(0)):\n",
        "                    fake_swa_pil.append(to_pil(fake_swa[k, :3].cpu().clamp(-1, 1)))\n",
        "\n",
        "    # Compute CMMDs\n",
        "    clip_full = compute_cmmd(real_images, fake_full_pil, is_clip=True)\n",
        "    clip_swa = compute_cmmd(real_images, fake_swa_pil, is_clip=True) if epoch >= swa_start else float('nan')\n",
        "    clip_cmmd_full.append(clip_full)\n",
        "    clip_cmmd_swa.append(clip_swa)\n",
        "\n",
        "    siglip_full = compute_cmmd(real_images, fake_full_pil, siglip_model, siglip_processor)\n",
        "    siglip_swa = compute_cmmd(real_images, fake_swa_pil, siglip_model, siglip_processor) \\\n",
        "        if epoch >= swa_start else siglip_full\n",
        "    siglip_cmmd_full.append(siglip_full)\n",
        "    siglip_cmmd_swa.append(siglip_swa)\n",
        "\n",
        "    align_full = compute_cmmd(real_images, fake_full_pil, align_model, align_processor)\n",
        "    align_swa = compute_cmmd(real_images, fake_swa_pil, align_model, align_processor) \\\n",
        "        if epoch >= swa_start else align_full\n",
        "    align_cmmd_full.append(align_full)\n",
        "    align_cmmd_swa.append(align_swa)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}:\")\n",
        "    print(f\"CLIP   CMMD (Full): {clip_full:.4f}, CMMD (SWA): {clip_swa:.4f}\")\n",
        "    print(f\"SIGLIP CMMD (Full): {siglip_full:.4f}, CMMD (SWA): {siglip_swa:.4f}\")\n",
        "    print(f\"ALIGN  CMMD (Full): {align_full:.4f}, CMMD (SWA): {align_swa:.4f}\\n\")\n",
        "\n",
        "# Plotting\n",
        "epochs = list(range(num_epochs))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, smooth(siglip_cmmd_full), label='SIGLIP CMMD Full', color='green', marker='o')\n",
        "plt.plot(epochs, smooth(siglip_cmmd_swa), label='SIGLIP CMMD SWA', color='lime', marker='s')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"CMMD Score\")\n",
        "plt.title(\"SIGLIP CMMD vs Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, smooth(align_cmmd_full), label='ALIGN CMMD Full', color='purple', marker='o')\n",
        "plt.plot(epochs, smooth(align_cmmd_swa), label='ALIGN CMMD SWA', color='magenta', marker='s')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"CMMD Score\")\n",
        "plt.title(\"ALIGN CMMD vs Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je-I4Y4WtV5Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Q1pzPnJFTVyZ",
        "03beP1qhTPUO",
        "QFm2aLUER5zE"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}